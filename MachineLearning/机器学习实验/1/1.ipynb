{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8746c0ef042da0e1",
   "metadata": {},
   "source": [
    "# 实验1 贝叶斯分类和knn"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "本次实验需要的所有包：",
   "id": "b7eb16514c812750"
  },
  {
   "cell_type": "code",
   "id": "5d7340239d5a197f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:50:34.615208Z",
     "start_time": "2024-05-21T18:50:34.595667Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba, os\n",
    "import random\n",
    "import matplotlib\n",
    "from IPython.display import display"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "id": "668a56b5034bb4c4",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "*自然语言处理？*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc428c998e0b619b",
   "metadata": {},
   "source": [
    "1.输入语料库并处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423e337caafad89",
   "metadata": {},
   "source": "  读取本地语料库。具体方法为从主文件开始读取子文件夹并读取子文件夹中的每一个txt的路径并输入到处理函数中"
  },
  {
   "cell_type": "code",
   "id": "5286188deb879582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:02:02.317627Z",
     "start_time": "2024-05-21T19:02:02.278079Z"
    }
   },
   "source": [
    "#读取所有子文件夹\n",
    "def getdir(folder):\n",
    "    subdir = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for dir in dirs:\n",
    "            subdir.append(os.path.join(root, dir))\n",
    "    return subdir\n",
    "#读入本文件夹所有的txt文件名\n",
    "def getdata(path):\n",
    "    txt_file = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                txt_file.append(os.path.join(root, file))\n",
    "    return txt_file\n",
    "folder = 'D:/机器学习实验/1/data'\n",
    "txt_file = [getdata(path) for path in getdir(folder)]\n",
    "label = np.array(['art']*742+['literature']* 34+ ['education']*61+['philosophy']*45 +['history']*468)"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "markdown",
   "id": "f5fbb03c652e8464",
   "metadata": {},
   "source": "### 1.处理文本内容"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.读入文件，用jieba划分词，读取blacklist进行停用词筛选。\n",
    "\n",
    "*并不用统计出现次数。。只需要统计有没有*"
   ],
   "id": "6f0ab2dbf76e6384"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:26:19.574835Z",
     "start_time": "2024-05-21T16:26:19.558702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#处理文本内容\n",
    "#预测结果不准可能是噪声过多\n",
    "from collections import defaultdict\n",
    "def process(filename, orig):\n",
    "    global blacklist, result\n",
    "    op = []\n",
    "    \n",
    "    # 打开文件并读取每一行\n",
    "    with open(filename, 'r', encoding='ansi', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # 处理每一行\n",
    "    for line in lines:\n",
    "        seg_list = jieba.cut(line.strip(), cut_all=False)\n",
    "        for seg in seg_list:\n",
    "            if seg not in blacklist:\n",
    "                if seg not in result:\n",
    "                    # 创建词的唯一索引（这里用 defaultdict 会自动处理）\n",
    "                    result[seg] = len(result)\n",
    "                # 把原文输入\n",
    "                op.append(seg)\n",
    "    \n",
    "    orig.append(op)\n",
    "       "
   ],
   "id": "8baa50231473ea13",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.读入停用词库(blacklist)",
   "id": "419618a4b52038e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:34:37.290777Z",
     "start_time": "2024-05-21T16:26:19.574835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#停用词库，参考哈工大、百度等停用词库 针对本实验进行调整\n",
    "def load_blacklist(blacklist_file):\n",
    "    blacklist = []\n",
    "    with open(blacklist_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            blacklist.append(line.rstrip('\\n'))\n",
    "    return blacklist\n",
    "blacklist = load_blacklist('D:/机器学习实验/1/blacklist.txt')\n",
    "result = defaultdict(lambda: len(result))\n",
    "# txt_file = getdata('D:/机器学习实验/1/test')\n",
    "orig = []\n",
    "for i in txt_file:\n",
    "    for j in i:\n",
    "        process(j, orig)"
   ],
   "id": "f37af9117466123a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.构建onehot文本向量\n",
    "\n",
    "这种方法便于计算欧式距离。\n",
    "\n",
    "onehot表示的二进制向量：这个词是否出现\n",
    "\n",
    "**处理后的原文和特征词比较，出现：1**"
   ],
   "id": "3638731b20bc3eb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**使用np.array存数组，pd.dataframe对应向量与标签**",
   "id": "cd4bcdf30079f08b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:34:12.822377Z",
     "start_time": "2024-05-21T18:33:57.156129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#使用onehot进行编码。每个文件生成一个向量数组，\n",
    "#根据字典总结出特征词（关键字），再用向量标识\n",
    "# 输入到数据集中再进行划分？？\n",
    "def encode(orig): \n",
    "    sum = []\n",
    "    for i in orig:\n",
    "        vector = [0] * len(result)\n",
    "        for j in i:\n",
    "            if j in result:\n",
    "                vector[result[j]] = 1\n",
    "        sum.append(vector)\n",
    "    return sum\n",
    "dataset = np.array(encode(orig))\n",
    "print(dataset)\n",
    "# df = pd.DataFrame({'vector': list(dataset), 'label': label})\n",
    "# print(df)"
   ],
   "id": "f34dd95ebbd0fcef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.划分测试集与训练集\n",
    "\n",
    "*不建议使用scikit-learn*"
   ],
   "id": "157d1d3d3f16f3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:14:05.982370Z",
     "start_time": "2024-05-21T19:14:05.689620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#设置测试集比例\n",
    "test_size = 0.3\n",
    "data_size = len(dataset)\n",
    "test_size = int(data_size * test_size)\n",
    "#生成随机索引\n",
    "indices = list(range(data_size))\n",
    "random.shuffle(indices)\n",
    "\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:]\n",
    "\n",
    "x_train = dataset[train_indices]\n",
    "y_train = label[train_indices]\n",
    "\n",
    "x_test = dataset[test_indices]\n",
    "y_test = label[test_indices]"
   ],
   "id": "e002088227e6917",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 学习模型构建",
   "id": "e64ef278611c3396"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### knn:\n",
    "\n",
    "选出前k个最近的向量与标签，比较标签的数量判断分类\n"
   ],
   "id": "50a3a354e238030b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:12:55.133317Z",
     "start_time": "2024-05-21T19:08:46.974193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KNNWithProbabilities:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def dist(self, x1, x2):\n",
    "        return np.sqrt(np.sum(np.square(x1 - x2), axis=1))\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for x in x_test:\n",
    "            # 计算测试样本与所有训练样本之间的距离\n",
    "            distances = self.dist(self.x_train, x)\n",
    "            # 找到距离最近的k个点的索引\n",
    "            nearest_ids = np.argsort(distances)[:self.k]\n",
    "            # 找到这些点的标签\n",
    "            nearest_labels = self.y_train[nearest_ids]\n",
    "            # 统计最近k个点中每个标签出现的次数\n",
    "            unique, counts = np.unique(nearest_labels, return_counts=True)\n",
    "            # 计算概率\n",
    "            class_probabilities = counts / self.k\n",
    "            probabilities.append(class_probabilities)\n",
    "            # 找到出现次数最多的标签\n",
    "            prediction = unique[np.argmax(counts)]\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "knn_with_probabilities = KNNWithProbabilities(k=20)\n",
    "knn_with_probabilities.fit(x_train, y_train)\n",
    "knn_prediction, knn_probabilities = knn_with_probabilities.predict(x_test)\n"
   ],
   "id": "1f9621365912b16b",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 朴素贝叶斯算法\n",
    "\n",
    "1.计算先验概率P(c): 训练集中每个类别的比例\n",
    "\n",
    "2.计算条件概率P(x|C) C中x出现的频率，使用拉普拉斯平滑处理\n",
    "\n",
    "3.利用条件概率分类，p(a1|b1b2...) > p(a2|b1b2...) a1 "
   ],
   "id": "fe27aaf2046d1e12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:13:09.030532Z",
     "start_time": "2024-05-21T19:12:55.133317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.parameters = None\n",
    "        self.prior_probabilities = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.classes = np.unique(y_train)\n",
    "        self.parameters = []\n",
    "        self.prior_probabilities = {}\n",
    "\n",
    "        for c in self.classes:\n",
    "            x_c = x_train[y_train == c]\n",
    "            self.parameters.append({\n",
    "                \"mean\": x_c.mean(axis=0),\n",
    "                \"var\": x_c.var(axis=0) + 1e-9  # Add epsilon to avoid division by zero\n",
    "            })\n",
    "            self.prior_probabilities[c] = len(x_c) / len(x_train)\n",
    "\n",
    "    def log_pdf(self, x, mean, var):\n",
    "        log_coefficient = -0.5 * np.log(2 * np.pi * var)\n",
    "        log_exponent = -((x - mean) ** 2 / (2 * var))\n",
    "        return log_coefficient + log_exponent\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        log_prior = np.log(np.array([self.prior_probabilities[c] for c in self.classes]))\n",
    "        probabilities = []\n",
    "        predictions = []\n",
    "\n",
    "        for x in x_test:\n",
    "            log_posterior_probs = []\n",
    "            class_probabilities = []\n",
    "            for i, c in enumerate(self.classes):\n",
    "                mean = self.parameters[i][\"mean\"]\n",
    "                var = self.parameters[i][\"var\"]\n",
    "                log_likelihood = np.sum(self.log_pdf(x, mean, var))\n",
    "                log_posterior = log_likelihood + log_prior[i]\n",
    "                log_posterior_probs.append(log_posterior)\n",
    "                class_probabilities.append(log_posterior)\n",
    "            posterior_probs = np.exp(log_posterior_probs - np.max(log_posterior_probs))  # 避免数值溢出\n",
    "            probabilities.append(posterior_probs / np.sum(posterior_probs))  # 归一化得到概率\n",
    "            predictions.append(self.classes[np.argmax(class_probabilities)])\n",
    "\n",
    "        return predictions, np.array(probabilities)\n",
    "\n",
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes.fit(x_train, y_train)\n",
    "predictions, probabilities = naive_bayes.predict(x_test)"
   ],
   "id": "422f08ec770f0339",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 评价模型的效果（进行3次实验）\n",
    "\n",
    "1.计算出两个模型的P R F1"
   ],
   "id": "721ed10ee4a9d91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:13:09.064356Z",
     "start_time": "2024-05-21T19:13:09.030532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 计算 Naive Bayes 模型的精确率、召回率和 F1 分数\n",
    "nb_precision = precision_score(y_test, predictions, average='macro', zero_division=0)\n",
    "nb_recall = recall_score(y_test, predictions, average='macro', zero_division=0)\n",
    "nb_f1 = f1_score(y_test, predictions, average='macro', zero_division=0)\n",
    "\n",
    "# 计算 KNN 模型的精确率、召回率和 F1 分数\n",
    "knn_precision = precision_score(y_test, knn_prediction, average='macro', zero_division=0)\n",
    "knn_recall = recall_score(y_test, knn_prediction, average='macro', zero_division=0)\n",
    "knn_f1 = f1_score(y_test, knn_prediction, average='macro', zero_division=0)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Naive Bayes - Precision: {nb_precision:.4f}, Recall: {nb_recall:.4f}, F1: {nb_f1:.4f}\")\n",
    "print(f\"KNN - Precision: {knn_precision:.4f}, Recall: {knn_recall:.4f}, F1: {knn_f1:.4f}\")"
   ],
   "id": "e5d5d5917d2e1342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Precision: 0.5127, Recall: 0.3511, F1: 0.3483\n",
      "KNN - Precision: 0.1941, Recall: 0.2875, F1: 0.2294\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.绘制ROC曲线",
   "id": "d885a140e05f7db3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405,)\n",
      "(405, 5)\n"
     ]
    }
   ],
   "execution_count": 127,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 将字符串标签转换为二进制形式\n",
    "class_mapping = {\"art\": 0, \"literature\": 1, \"education\": 2, \"philosophy\": 3, \"history\": 4}\n",
    "y_true_binary = np.zeros((len(y_test), len(class_mapping)))\n",
    "for i, l in enumerate(y_test):\n",
    "    for key, value in class_mapping.items():\n",
    "        if key in l:\n",
    "            y_true_binary[i, value] = 1\n",
    "            \n",
    "knn_probabilities = np.array(knn_probabilities, dtype='object')\n",
    "probabilities = np.array(probabilities, dtype='object')\n",
    "print(knn_probabilities.shape)\n",
    "print(probabilities.shape)"
   ],
   "id": "2855d32e9553a3a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:13:09.165391Z",
     "start_time": "2024-05-21T19:13:09.078936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 计算 KNN 模型的 ROC 曲线\n",
    "fpr_model1, tpr_model1, _ = roc_curve(y_true_binary.ravel(), knn_probabilities.ravel())\n",
    "roc_auc_model1 = auc(fpr_model1, tpr_model1)\n",
    "\n",
    "# 计算 Naive Bayes 模型的 ROC 曲线\n",
    "fpr_model2, tpr_model2, _ = roc_curve(y_true_binary.ravel(), probabilities.ravel())\n",
    "roc_auc_model2 = auc(fpr_model2, tpr_model2)\n",
    "\n",
    "# 绘制 ROC 曲线\n",
    "plt.figure()\n",
    "plt.plot(fpr_model1, tpr_model1, color='darkorange', lw=2, label='Model 1 (AUC = %0.2f)' % roc_auc_model1)\n",
    "plt.plot(fpr_model2, tpr_model2, color='green', lw=2, label='Model 2 (AUC = %0.2f)' % roc_auc_model2)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "id": "36e29a6d61eae17d",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2025, 405]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[128], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 计算 KNN 模型的 ROC 曲线\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m fpr_model1, tpr_model1, _ \u001B[38;5;241m=\u001B[39m roc_curve(y_true_binary\u001B[38;5;241m.\u001B[39mravel(), knn_probabilities\u001B[38;5;241m.\u001B[39mravel())\n\u001B[0;32m      3\u001B[0m roc_auc_model1 \u001B[38;5;241m=\u001B[39m auc(fpr_model1, tpr_model1)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# 计算 Naive Bayes 模型的 ROC 曲线\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:992\u001B[0m, in \u001B[0;36mroc_curve\u001B[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001B[0m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mroc_curve\u001B[39m(\n\u001B[0;32m    905\u001B[0m     y_true, y_score, \u001B[38;5;241m*\u001B[39m, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, drop_intermediate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    906\u001B[0m ):\n\u001B[0;32m    907\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001B[39;00m\n\u001B[0;32m    908\u001B[0m \n\u001B[0;32m    909\u001B[0m \u001B[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    990\u001B[0m \u001B[38;5;124;03m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001B[39;00m\n\u001B[0;32m    991\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 992\u001B[0m     fps, tps, thresholds \u001B[38;5;241m=\u001B[39m _binary_clf_curve(\n\u001B[0;32m    993\u001B[0m         y_true, y_score, pos_label\u001B[38;5;241m=\u001B[39mpos_label, sample_weight\u001B[38;5;241m=\u001B[39msample_weight\n\u001B[0;32m    994\u001B[0m     )\n\u001B[0;32m    996\u001B[0m     \u001B[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001B[39;00m\n\u001B[0;32m    997\u001B[0m     \u001B[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001B[39;00m\n\u001B[0;32m    998\u001B[0m     \u001B[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1003\u001B[0m     \u001B[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001B[39;00m\n\u001B[0;32m   1004\u001B[0m     \u001B[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001B[39;00m\n\u001B[0;32m   1005\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_intermediate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fps) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001B[0m, in \u001B[0;36m_binary_clf_curve\u001B[1;34m(y_true, y_score, pos_label, sample_weight)\u001B[0m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m pos_label \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[0;32m    749\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m format is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(y_type))\n\u001B[1;32m--> 751\u001B[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001B[0;32m    752\u001B[0m y_true \u001B[38;5;241m=\u001B[39m column_or_1d(y_true)\n\u001B[0;32m    753\u001B[0m y_score \u001B[38;5;241m=\u001B[39m column_or_1d(y_score)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [2025, 405]"
     ]
    }
   ],
   "execution_count": 128
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
